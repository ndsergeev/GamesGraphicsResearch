# SELF PLAY, FOR TWO TEAMS
behaviors:
    MLRaySelfPlay:
        trainer_type: ppo
        hyperparameters:
            batch_size: 2048
            buffer_size: 20480
            learning_rate: 3.0e-4
            beta: 1e-4
            epsilon: 0.2
            lambd: 0.95 # 0.99
            num_epoch: 3
            learning_rate_schedule: constant
        network_settings:
            normalize: false # true
            hidden_units: 512 # 128
            num_layers: 2
            vis_encode_type: simple
        reward_signals:
            extrinsic:
                gamma: 0.99
                strength: 1.0
        keep_checkpoints: 5
        max_steps: 5.0e6
        time_horizon: 1000
        summary_freq: 2500
        threaded: true
        self_play:
            save_steps: 50000
            team_change: 200000
            swap_steps: 1000
            window: 10
            play_against_latest_model_ratio: 0.5
            initial_elo: 1200.0
